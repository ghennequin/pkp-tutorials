{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal models for vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ";;\n",
    "#require \"pkp\"\n",
    "\n",
    "open Owl\n",
    "\n",
    "open Pkp.Visual_coding\n",
    "\n",
    "let _ = Pkp.Misc.quiet_owl ()\n",
    "\n",
    "let data_dir = \"/home/opam/pkp/pkp-tutorials/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will investigate two candidate “internal models” for the perception of small patches of natural images.\n",
    "\n",
    "As discussed in class, our brain gets an input image $s$ from the retina (two, actually, but we'll neglect stereo vision here!), and must infer the underlying physical “causes” $x$ (features in the environment) that might have given rise to that image $s$. Given an “internal model of the world” comprised of a prior $p(x)$ over features, and a likelihood function $p(s|x)$ describing the optics / response properties of the eye (i.e. how specific environmental features $x$ give rise to images $s$), all the brain needs to do is compute the so-called posterior distribution $p(x|s)$ : the result of **perception**.\n",
    "\n",
    "During the lecture, the question arose as to what exactly are those “features of the environment” that our brain should infer. For example, how do we know that the world contains chairs and tables, and that inferring their presence in a scene helps us explain the images we get from our eyes? In this notebook, we consider the problem of learning such internal models. For tractability, we will focus on trying to discover latent causes for small patches of natural images, and work with a simple family of probabilistic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The data\n",
    "\n",
    "Let's start by visualising the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let raw_imgs = load_images ~file:(data_dir ^ \"natural_images_raw.bin\") ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the shape of `raw_imgs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let _ = Arr.shape raw_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's a 3D array, i.e. an array of 9 images, each represented as a 512x512 matrix.\n",
    "\n",
    "Run the code below for a high-tech movie of those 9 images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let _ =\n",
    "  let ph = placeholder () in\n",
    "  Arr.iter_slice\n",
    "    ~axis:0\n",
    "    (fun img ->\n",
    "      plot_image ~ph img;\n",
    "      Unix.sleepf 1.0)\n",
    "    raw_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the rest of this notebook, we will work with pre-processed version of these images. Specifically, preprocessing is applied that mimics the filtering properties of retinal ganglion cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let imgs = load_images ~file:(data_dir ^ \"natural_images.bin\") ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let _ =\n",
    "  let ph = placeholder () in\n",
    "  Arr.iter_slice\n",
    "    ~axis:0\n",
    "    (fun img ->\n",
    "      plot_image ~ph img;\n",
    "      Unix.sleepf 1.0)\n",
    "    imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess how, technically, these images were obtained from the raw images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning algorithms below will need to randomly sample a lot of image patches (14x14) from the 9 big images in `imgs`. For this, they will need a `stream`, which is basically an “infinite fountain” of natural image patches.\n",
    "\n",
    "**TODO**: create a “data stream” from those `imgs` using the `create_stream` function, \n",
    "    and visualise the stream using `visualise_stream` (see [documentation](https://pkp-neuro.github.io/pkp-tutorials/pkp/Pkp/Visual_coding/index.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(* your code here *)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Internal models of small patches\n",
    "\n",
    "The family of models we are going to investigate postulate that any retinal image $s$ arises from the noisy linear superposition of a fixed set of “prototypical images” $p_i$ ($i=1,2,\\ldots,K$) of the same size as $s$ (14x14 pixels), each weighted by some intensity $x_i$:\n",
    "\n",
    "$$ s = \\sum_i x_i p_i  + \\text{noise} $$\n",
    "\n",
    "where $\\text{noise}$ is some random Gaussian noise that corrupts the image. The prototypical images $p_i$ are the same for all image patches, but their intensities $x_i$ differ from image to image. Perception is about inferring the $x_i$ that might have given rise to a particular $s$. We assume that, before even observing an image patch, we have a prior belief $p(x)$ over what each $x_i$ might be. Together, $p(x)$ and the $p_i$ templates form our “internal model”.\n",
    "\n",
    "Think of each $p_i$ as a possible “local feature” of the visual scene, and $x_i$ as the intensity with which it contributes to the given image patch $s$. The $x_i$ are thought to be represented in neural activity (here, we're agnostic to exactly how), and therefore will be subject to (e.g. energy, resources) constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dense coding models\n",
    "\n",
    "We will begin by considering “dense coding models”, i.e. a family of models that assume that the feature intensities are normally distributed (i.e. following a Gaussian distribution) _a priori_ (i.e. $p(x)$ is a Gaussian distribution). The reason why they called “dense” models is because samples from a Gaussian distribution are “densely spread“ around the mean. This will be contrasted later with “sparse models” with highly non-Gaussian prior distributions $p(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let _ =\n",
    "  let fig (module P : Plot) =\n",
    "    let xs = Mat.linspace (-6.) 6. 400 in\n",
    "    let dense = Owl_stats.gaussian_pdf ~mu:0. ~sigma:1. in\n",
    "    let sparse = Owl_stats.laplace_pdf ~loc:0. ~scale:1. in\n",
    "    P.plots\n",
    "      [ item (F (dense, xs)) ~legend:\"dense\" ~style:\"l lc 7 lw 2\"\n",
    "      ; item (F (sparse, xs)) ~legend:\"sparse\" ~style:\"l lc 3 lw 2\"\n",
    "      ]\n",
    "      [ barebone\n",
    "      ; borders [ `bottom; `left ]\n",
    "      ; xtics (`regular [ -10.; 2. ])\n",
    "      ; ytics `auto\n",
    "      ; xlabel \"x\"\n",
    "      ; ylabel \"density\"\n",
    "      ]\n",
    "  in\n",
    "  Juplot.draw ~size:(400, 300) fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Try adding `set \"log y\"` to the list of plot properties above (e.g. after `barebone`), to plot these two distributions on a logarithmic y-axis. What do you notice?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 A random dense model\n",
    "\n",
    "Let's get warmed up with the library by looking at a model with 25 completely random features. This will be a bad model and will motivate learning of better ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let proto = Arr.gaussian [| 25; 14; 14 |]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let _ = plot_patches proto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convince ourselves that this is a bad model of natural image patches, we can do the following:\n",
    "1. sample a few image patches s from our stream of natural image patches\n",
    "2. figure out the most likely feature intensities $x_i$, for each image patch\n",
    "3. try and “reconstruct the image patches“ according the equation above, using those most likely feature intensities $x_i$ found in step 2 ─ and compare to the original patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: sample 16 image patches from the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let s = sample_stream stream 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let _ = Arr.shape s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let _ = plot_patches s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: infer the most likely feature intensities for these 16 patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let x_best = Dense_model.most_likely_intensities proto s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let _ = Arr.shape x_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: attempt to reconstruct each $s$ as $\\sum_i x_i p_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let s_reconstructed = reconstruct proto x_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let _ = plot_patches s_reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comparison should look pretty awful. In other words, densely combined random features are a poor description of natural images, which are in fact much more structured. Try increasing the number of random prototypical patches in the model (it was set to 25 above). How many “random features“ do you need to have a decent-looking reconstruction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Learning a better dense model\n",
    "\n",
    "Now, we are going to optimise the prototypical patches in our dense model, so that the distribution of model-generated patches becomes progressively more similar to the empirical distribution of image patches given by our stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let proto = Dense_model.learn stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a few minutes. Observe the process!\n",
    "\n",
    "Now, with this new optimised set of prototypical patches, try to run the same reconstruction analysis as above. Are these 25 templates any better than the randomly-generated ones?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse coding model\n",
    "\n",
    "We are now going to learn a model in which the feature intensities are distributed in a sparse way. We will consider a larger bank of prototypical features ($K=100$, compared to $25$ above), but require that, statistically, only few of these features be used in any given image. In other words, $p(x)$ is such that each $x_i$ is very often very small, but occasionally very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let proto = Sparse_model.learn stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice? How do these features compare with the receptive fields of neurons in the primary visual cortex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisit the reconstruction analysis performed above. Is sparse coding an efficient way of coding natural images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the distribution of “most likely feature intensities” look like under this new sparse model? Compare with the dense coding model. Given a matrix `x` of intensities, you can plot a histogram with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let plot_intensity_hist x =\n",
    "  let open Gp in\n",
    "  let fig (module P : Plot) =\n",
    "    P.plot\n",
    "      (A Pkp.Misc.(hist ~n_bins:50 x))\n",
    "      ~style:\"boxes fs solid 0.5 lc 8\"\n",
    "      [ barebone; borders [ `bottom ]; xtics `auto; xlabel \"intensity\" ]\n",
    "  in\n",
    "  Juplot.draw ~size:(400, 300) fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCaml 4.07.1+flambda",
   "language": "OCaml",
   "name": "ocaml-jupyter"
  },
  "language_info": {
   "codemirror_mode": "text/x-ocaml",
   "file_extension": ".ml",
   "mimetype": "text/x-ocaml",
   "name": "OCaml",
   "nbconverter_exporter": null,
   "pygments_lexer": "OCaml",
   "version": "4.07.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
