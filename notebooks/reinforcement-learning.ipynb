{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ";;\n",
    "#require \"pkp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open Pkp.Reinforcement_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module MySolution = struct\n",
    "  let other_mark = Solution.other_mark\n",
    "  let empty_board = Solution.empty_board\n",
    "  let transpose = Solution.transpose\n",
    "  let is_full = Solution.is_full\n",
    "  let mean = Solution.mean\n",
    "  let play_game = Solution.play_game\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module M = Make (MySolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run an example game between the optimal X player, and a random O player:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let result = M.play (M.random O, M.optimal X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running it several times to convince yourself that the optimal player does pretty well! Try also reversing the order of the players, so that the O player gets to start. What do you notice? Does the optimal player ever lose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Let us now define an O-player that interpolates between the optimal player (which is as strong as our optimal X-player) and a random player: every time it needs to chose a new board, it makes a random choice with probability $p$, and the optimal choice with probability $(1-p)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let playerO p =\n",
    " let dumb = M.random O in\n",
    " let opt = M.optimal O in\n",
    " let play b = if Random.float 1. < p then dumb.play b else opt.play b in\n",
    " { mark = O; play }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try out such an intermediate player -- you may want to play with parameter $p$, and swap the player ordering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let result = M.play (playerO 0.5, playerX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now run many games, collect some statistics, and make pretty plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let stats p =\n",
    "  let playerO = playerO p in\n",
    "  let n_games = 10000 in\n",
    "  let games =   List.init n_games (fun _ -> M.play ~display:false (playerO, playerX)) in\n",
    "  let n_wins = games |> List.filter (fun winner -> winner = Some X) |> List.length in\n",
    "  let n_ties = games |> List.filter (fun winner -> winner = None) |> List.length in\n",
    "  float n_wins /. float n_games, float n_ties /. float n_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "let () =\n",
    "  let open Owl in\n",
    "  let ps = Mat.linspace 0. 1. 20 in\n",
    "  let results = ps |> Mat.to_array |> Array.map stats in\n",
    "  let wins = results |> Array.map fst |> fun v -> Mat.of_array v 1 (-1) in\n",
    "  let ties = results |> Array.map snd |> fun v -> Mat.of_array v 1 (-1) in\n",
    "  let lose = Mat.(1. $- wins + ties) in\n",
    "  let open Gp in\n",
    "  let figure (module P : Plot) =\n",
    "    P.plots\n",
    "      [ item (L [ ps; wins ]) ~style:\"lp pt 7 lc 7 ps 0.6\" ~legend:\"win\"\n",
    "      ; item (L [ ps; ties ]) ~style:\"lp pt 7 lc 8 ps 0.6\" ~legend:\"tie\"\n",
    "      ; item (L [ ps; lose ]) ~style:\"lp pt 7 lc 3 ps 0.6\" ~legend:\"lose\"\n",
    "      ]\n",
    "      [ barebone\n",
    "      ; set \"key at graph 1.1, graph 1 top left\"\n",
    "      ; tics \"out nomirror\"\n",
    "      ; borders [ `bottom; `left ]\n",
    "      ; xlabel \"probability of opponent playing randomly\"\n",
    "      ; ylabel \"win / tie probabilities\"\n",
    "      ; margins [ `right 0.6 ]\n",
    "      ]\n",
    "  in\n",
    "  Juplot.draw ~fmt:`svg ~size:(500, 200) figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take-home exercise: redo the stats above, in the case where the order of play gets decided randomly (50-50) at the beginning of every game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCaml 4.07.1+flambda",
   "language": "OCaml",
   "name": "ocaml-jupyter"
  },
  "language_info": {
   "codemirror_mode": "text/x-ocaml",
   "file_extension": ".ml",
   "mimetype": "text/x-ocaml",
   "name": "OCaml",
   "nbconverter_exporter": null,
   "pygments_lexer": "OCaml",
   "version": "4.07.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
